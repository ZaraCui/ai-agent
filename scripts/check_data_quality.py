#!/usr/bin/env python3
"""
å…¨é¢çš„æ™¯ç‚¹æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬
æ£€æŸ¥é—®é¢˜ï¼š
1. é‡å¤æ™¯ç‚¹ï¼ˆåŸºäºåç§°ç›¸ä¼¼åº¦å’Œåæ ‡è·ç¦»ï¼‰
2. åæ ‡é‡å¤
3. æè¿°ç›¸ä¼¼åº¦
4. ç¼ºå¤±å­—æ®µ
5. æ— æ•ˆæ•°æ®
"""
import json
from pathlib import Path
from collections import defaultdict
import math
from difflib import SequenceMatcher

def calculate_distance(lat1, lon1, lat2, lon2):
    """è®¡ç®—ä¸¤ä¸ªåæ ‡ç‚¹ä¹‹é—´çš„è·ç¦»ï¼ˆç±³ï¼‰"""
    R = 6371000  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰
    phi1 = math.radians(lat1)
    phi2 = math.radians(lat2)
    delta_phi = math.radians(lat2 - lat1)
    delta_lambda = math.radians(lon2 - lon1)
    
    a = math.sin(delta_phi/2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    
    return R * c

def name_similarity(name1, name2):
    """è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    # ç§»é™¤å¸¸è§çš„å‰ç¼€/åç¼€
    clean1 = name1.replace('åŒ—äº¬', '').replace('æ•…å®«åšç‰©é™¢-', '').replace('å¤©å›å…¬å›­-', '').strip()
    clean2 = name2.replace('åŒ—äº¬', '').replace('æ•…å®«åšç‰©é™¢-', '').replace('å¤©å›å…¬å›­-', '').strip()
    
    # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
    return SequenceMatcher(None, clean1, clean2).ratio()

def check_duplicate_spots(spots, city_name, threshold_name=0.8, threshold_distance=100):
    """
    æ£€æŸ¥é‡å¤æ™¯ç‚¹
    threshold_name: åç§°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
    threshold_distance: è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰
    """
    duplicates = []
    
    for i in range(len(spots)):
        for j in range(i + 1, len(spots)):
            spot1 = spots[i]
            spot2 = spots[j]
            
            name1 = spot1.get('name', '')
            name2 = spot2.get('name', '')
            
            # æ£€æŸ¥åç§°ç›¸ä¼¼åº¦
            similarity = name_similarity(name1, name2)
            
            # æ£€æŸ¥åæ ‡è·ç¦»
            lat1, lon1 = spot1.get('lat'), spot1.get('lon')
            lat2, lon2 = spot2.get('lat'), spot2.get('lon')
            
            distance = None
            if all([lat1, lon1, lat2, lon2]):
                distance = calculate_distance(lat1, lon1, lat2, lon2)
            
            # åˆ¤æ–­æ˜¯å¦é‡å¤
            is_duplicate = False
            reason = []
            
            if similarity >= threshold_name:
                is_duplicate = True
                reason.append(f"åç§°ç›¸ä¼¼åº¦ {similarity:.2%}")
            
            if distance is not None and distance <= threshold_distance:
                is_duplicate = True
                reason.append(f"è·ç¦» {distance:.0f}ç±³")
            
            if is_duplicate:
                duplicates.append({
                    'spot1': name1,
                    'spot2': name2,
                    'similarity': similarity,
                    'distance': distance,
                    'reason': ', '.join(reason),
                    'index1': i,
                    'index2': j
                })
    
    return duplicates

def check_description_quality(spots, city_name):
    """æ£€æŸ¥æè¿°è´¨é‡"""
    issues = []
    
    for i, spot in enumerate(spots):
        name = spot.get('name', '')
        desc = spot.get('description', '')
        
        # æ£€æŸ¥ç©ºæè¿°
        if not desc or len(desc.strip()) == 0:
            issues.append({
                'type': 'ç©ºæè¿°',
                'spot': name,
                'index': i
            })
            continue
        
        # æ£€æŸ¥è¿‡çŸ­æè¿°
        if len(desc) < 50:
            issues.append({
                'type': 'æè¿°è¿‡çŸ­',
                'spot': name,
                'index': i,
                'length': len(desc)
            })
        
        # æ£€æŸ¥é‡å¤çš„æè¿°å¼€å¤´ï¼ˆå¯èƒ½æ˜¯æ¨¡æ¿ï¼‰
        if desc.startswith('åœ¨åŒ—äº¬') and desc.count('åœ¨åŒ—äº¬') > 1:
            issues.append({
                'type': 'æè¿°å¯èƒ½æœ‰é‡å¤æ–‡æœ¬',
                'spot': name,
                'index': i
            })
    
    return issues

def check_missing_info(spots, city_name):
    """æ£€æŸ¥ç¼ºå¤±çš„é‡è¦ä¿¡æ¯"""
    issues = []
    
    for i, spot in enumerate(spots):
        name = spot.get('name', '')
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ['name', 'category', 'duration_minutes', 'rating', 'lat', 'lon', 'description', 'city']
        missing = [f for f in required_fields if f not in spot or spot[f] is None]
        
        if missing:
            issues.append({
                'spot': name,
                'missing_fields': missing,
                'index': i
            })
    
    return issues

def check_data_anomalies(spots, city_name):
    """æ£€æŸ¥æ•°æ®å¼‚å¸¸ï¼ˆå¦‚å¼‚å¸¸è¯„åˆ†ã€æ—¶é•¿ç­‰ï¼‰"""
    issues = []
    
    for i, spot in enumerate(spots):
        name = spot.get('name', '')
        
        # æ£€æŸ¥è¯„åˆ†
        rating = spot.get('rating')
        if rating is not None:
            if not (1 <= rating <= 5):
                issues.append({
                    'type': 'è¯„åˆ†è¶Šç•Œ',
                    'spot': name,
                    'value': rating,
                    'index': i
                })
            elif rating == 4.0:  # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯4.0ï¼ˆå¯èƒ½æ˜¯é»˜è®¤å€¼ï¼‰
                pass  # æˆ‘ä»¬ä¼šåœ¨å¦ä¸€ä¸ªæ£€æŸ¥ä¸­ç»Ÿè®¡
        
        # æ£€æŸ¥æŒç»­æ—¶é—´
        duration = spot.get('duration_minutes')
        if duration is not None:
            if duration <= 0:
                issues.append({
                    'type': 'æŒç»­æ—¶é—´æ— æ•ˆ',
                    'spot': name,
                    'value': duration,
                    'index': i
                })
            elif duration > 480:  # è¶…è¿‡8å°æ—¶
                issues.append({
                    'type': 'æŒç»­æ—¶é—´å¼‚å¸¸é•¿',
                    'spot': name,
                    'value': duration,
                    'index': i
                })
        
        # æ£€æŸ¥ç±»åˆ«
        valid_categories = ['sightseeing', 'museum', 'outdoor', 'history', 'food', 'shopping', 'entertainment']
        category = spot.get('category')
        if category and category not in valid_categories:
            issues.append({
                'type': 'æœªçŸ¥ç±»åˆ«',
                'spot': name,
                'value': category,
                'index': i
            })
    
    return issues

def analyze_data_quality():
    """åˆ†ææ‰€æœ‰åŸå¸‚çš„æ•°æ®è´¨é‡"""
    data_dir = Path('data')
    
    print("=" * 80)
    print("ğŸ” æ™¯ç‚¹æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 80)
    
    all_issues = defaultdict(list)
    total_spots = 0
    total_cities = 0
    
    for json_file in sorted(data_dir.glob('spots_*.json')):
        city = json_file.stem.replace('spots_', '')
        total_cities += 1
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                spots = json.load(f)
        except json.JSONDecodeError as e:
            print(f"\nâŒ {city}: JSON è§£æé”™è¯¯ - {e}")
            continue
        
        total_spots += len(spots)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ åŸå¸‚: {city} ({len(spots)} ä¸ªæ™¯ç‚¹)")
        print(f"{'='*80}")
        
        # 1. æ£€æŸ¥é‡å¤æ™¯ç‚¹
        print(f"\nğŸ” æ£€æŸ¥é‡å¤æ™¯ç‚¹...")
        duplicates = check_duplicate_spots(spots, city)
        if duplicates:
            print(f"  âŒ å‘ç° {len(duplicates)} ç»„ç–‘ä¼¼é‡å¤æ™¯ç‚¹:")
            for dup in duplicates[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"    â€¢ {dup['spot1']} âŸ· {dup['spot2']}")
                print(f"      åŸå› : {dup['reason']}")
            if len(duplicates) > 10:
                print(f"    ... è¿˜æœ‰ {len(duplicates) - 10} ç»„é‡å¤")
            all_issues[f'{city}_duplicates'].extend(duplicates)
        else:
            print(f"  âœ… æœªå‘ç°é‡å¤æ™¯ç‚¹")
        
        # 2. æ£€æŸ¥ç¼ºå¤±ä¿¡æ¯
        print(f"\nğŸ“ æ£€æŸ¥ç¼ºå¤±å­—æ®µ...")
        missing = check_missing_info(spots, city)
        if missing:
            print(f"  âŒ å‘ç° {len(missing)} ä¸ªæ™¯ç‚¹ç¼ºå°‘å­—æ®µ:")
            for issue in missing[:5]:
                print(f"    â€¢ {issue['spot']}: ç¼ºå°‘ {', '.join(issue['missing_fields'])}")
            if len(missing) > 5:
                print(f"    ... è¿˜æœ‰ {len(missing) - 5} ä¸ª")
            all_issues[f'{city}_missing'].extend(missing)
        else:
            print(f"  âœ… æ‰€æœ‰æ™¯ç‚¹å­—æ®µå®Œæ•´")
        
        # 3. æ£€æŸ¥æè¿°è´¨é‡
        print(f"\nğŸ“– æ£€æŸ¥æè¿°è´¨é‡...")
        desc_issues = check_description_quality(spots, city)
        if desc_issues:
            print(f"  âš ï¸ å‘ç° {len(desc_issues)} ä¸ªæè¿°è´¨é‡é—®é¢˜:")
            type_counts = defaultdict(int)
            for issue in desc_issues:
                type_counts[issue['type']] += 1
            for issue_type, count in type_counts.items():
                print(f"    â€¢ {issue_type}: {count} ä¸ª")
            all_issues[f'{city}_descriptions'].extend(desc_issues)
        else:
            print(f"  âœ… æè¿°è´¨é‡è‰¯å¥½")
        
        # 4. æ£€æŸ¥æ•°æ®å¼‚å¸¸
        print(f"\nâš¡ æ£€æŸ¥æ•°æ®å¼‚å¸¸...")
        anomalies = check_data_anomalies(spots, city)
        if anomalies:
            print(f"  âš ï¸ å‘ç° {len(anomalies)} ä¸ªæ•°æ®å¼‚å¸¸:")
            type_counts = defaultdict(int)
            for issue in anomalies:
                type_counts[issue['type']] += 1
            for issue_type, count in type_counts.items():
                print(f"    â€¢ {issue_type}: {count} ä¸ª")
            all_issues[f'{city}_anomalies'].extend(anomalies)
        else:
            print(f"  âœ… æ•°æ®æ ¼å¼æ­£å¸¸")
        
        # 5. ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒï¼ˆæ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„åˆ†éƒ½ä¸€æ ·ï¼‰
        ratings = [s.get('rating') for s in spots if s.get('rating') is not None]
        if ratings:
            unique_ratings = set(ratings)
            if len(unique_ratings) == 1:
                print(f"  âš ï¸ æ‰€æœ‰æ™¯ç‚¹è¯„åˆ†éƒ½æ˜¯ {ratings[0]}ï¼ˆå¯èƒ½éœ€è¦æ›´æ–°ï¼‰")
            else:
                print(f"  â„¹ï¸ è¯„åˆ†åˆ†å¸ƒ: {dict(sorted([(r, ratings.count(r)) for r in unique_ratings]))}")
    
    # æ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print(f"{'='*80}")
    print(f"  æ€»åŸå¸‚æ•°: {total_cities}")
    print(f"  æ€»æ™¯ç‚¹æ•°: {total_spots}")
    print(f"  å¹³å‡æ¯åŸå¸‚: {total_spots / total_cities:.1f} ä¸ªæ™¯ç‚¹")
    
    # è®¡ç®—æ€»é—®é¢˜æ•°
    total_issues = sum(len(issues) for issues in all_issues.values())
    print(f"  å‘ç°é—®é¢˜æ€»æ•°: {total_issues}")
    
    # å»ºè®®
    print(f"\n{'='*80}")
    print(f"ğŸ’¡ æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®")
    print(f"{'='*80}")
    print(f"  1. å¤„ç†é‡å¤æ™¯ç‚¹ï¼šæ£€æŸ¥ä¸Šè¿°é‡å¤æ™¯ç‚¹ï¼Œå†³å®šä¿ç•™ã€åˆå¹¶æˆ–åˆ é™¤")
    print(f"  2. å®Œå–„ç¼ºå¤±å­—æ®µï¼šä¸ºç¼ºå°‘å­—æ®µçš„æ™¯ç‚¹è¡¥å……ä¿¡æ¯")
    print(f"  3. ä¼˜åŒ–æè¿°ï¼šæ”¹è¿›è¿‡çŸ­æˆ–é‡å¤çš„æè¿°")
    print(f"  4. å¤šæ ·åŒ–è¯„åˆ†ï¼šè€ƒè™‘ä½¿ç”¨æ›´çœŸå®çš„è¯„åˆ†æ•°æ®")
    print(f"  5. éªŒè¯åæ ‡ï¼šç¡®ä¿æ‰€æœ‰åæ ‡å‡†ç¡®æ— è¯¯")
    
    return all_issues

def generate_duplicate_report(city_name=None):
    """ç”Ÿæˆé‡å¤æ™¯ç‚¹çš„è¯¦ç»†æŠ¥å‘Š"""
    data_dir = Path('data')
    
    if city_name:
        files = [data_dir / f'spots_{city_name}.json']
    else:
        files = sorted(data_dir.glob('spots_*.json'))
    
    print("=" * 80)
    print("ğŸ” é‡å¤æ™¯ç‚¹è¯¦ç»†æŠ¥å‘Š")
    print("=" * 80)
    
    for json_file in files:
        city = json_file.stem.replace('spots_', '')
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                spots = json.load(f)
        except:
            continue
        
        duplicates = check_duplicate_spots(spots, city)
        
        if duplicates:
            print(f"\nğŸ“ åŸå¸‚: {city}")
            print(f"   å‘ç° {len(duplicates)} ç»„ç–‘ä¼¼é‡å¤æ™¯ç‚¹\n")
            
            for i, dup in enumerate(duplicates, 1):
                print(f"   {i}. æ™¯ç‚¹å¯¹æ¯”:")
                print(f"      æ™¯ç‚¹A: {dup['spot1']} (ç´¢å¼•: {dup['index1']})")
                print(f"      æ™¯ç‚¹B: {dup['spot2']} (ç´¢å¼•: {dup['index2']})")
                print(f"      åç§°ç›¸ä¼¼åº¦: {dup['similarity']:.2%}")
                if dup['distance']:
                    print(f"      è·ç¦»: {dup['distance']:.0f} ç±³")
                print(f"      åˆ¤æ–­ä¾æ®: {dup['reason']}")
                print()

def generate_cleanup_suggestions(city_name):
    """ä¸ºç‰¹å®šåŸå¸‚ç”Ÿæˆæ¸…ç†å»ºè®®"""
    data_dir = Path('data')
    json_file = data_dir / f'spots_{city_name}.json'
    
    if not json_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°åŸå¸‚æ•°æ®æ–‡ä»¶: {json_file}")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        spots = json.load(f)
    
    print("=" * 80)
    print(f"ğŸ› ï¸ {city_name} æ•°æ®æ¸…ç†å»ºè®®")
    print("=" * 80)
    
    # é‡å¤æ™¯ç‚¹å»ºè®®
    duplicates = check_duplicate_spots(spots, city_name)
    if duplicates:
        print(f"\n1ï¸âƒ£ é‡å¤æ™¯ç‚¹å¤„ç†å»ºè®® ({len(duplicates)} ç»„):")
        print("-" * 80)
        
        for i, dup in enumerate(duplicates, 1):
            spot1 = spots[dup['index1']]
            spot2 = spots[dup['index2']]
            
            print(f"\n  ç»„ {i}:")
            print(f"    æ™¯ç‚¹A [{dup['index1']}]: {dup['spot1']}")
            print(f"      ç±»åˆ«: {spot1.get('category')}, è¯„åˆ†: {spot1.get('rating')}")
            print(f"      æè¿°é•¿åº¦: {len(spot1.get('description', ''))} å­—ç¬¦")
            
            print(f"    æ™¯ç‚¹B [{dup['index2']}]: {dup['spot2']}")
            print(f"      ç±»åˆ«: {spot2.get('category')}, è¯„åˆ†: {spot2.get('rating')}")
            print(f"      æè¿°é•¿åº¦: {len(spot2.get('description', ''))} å­—ç¬¦")
            
            print(f"    å»ºè®®: ", end='')
            if dup['similarity'] > 0.9 and (dup['distance'] is None or dup['distance'] < 50):
                print("å¾ˆå¯èƒ½æ˜¯é‡å¤ï¼Œå»ºè®®åˆ é™¤å…¶ä¸­ä¸€ä¸ª")
            elif dup['similarity'] > 0.8:
                print("ç–‘ä¼¼é‡å¤ï¼Œéœ€è¦äººå·¥ç¡®è®¤")
            elif dup['distance'] and dup['distance'] < 100:
                print("ä½ç½®éå¸¸æ¥è¿‘ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€æ™¯ç‚¹çš„ä¸åŒåç§°")
            else:
                print("éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤")
    
    # å…¶ä»–è´¨é‡é—®é¢˜
    desc_issues = check_description_quality(spots, city_name)
    if desc_issues:
        print(f"\n2ï¸âƒ£ æè¿°è´¨é‡é—®é¢˜ ({len(desc_issues)} ä¸ª):")
        print("-" * 80)
        type_counts = defaultdict(list)
        for issue in desc_issues:
            type_counts[issue['type']].append(issue)
        
        for issue_type, items in type_counts.items():
            print(f"\n  {issue_type} ({len(items)} ä¸ª):")
            for item in items[:5]:
                print(f"    â€¢ [{item['index']}] {item['spot']}")
            if len(items) > 5:
                print(f"    ... è¿˜æœ‰ {len(items) - 5} ä¸ª")

def export_duplicates_json(output_file='output/duplicates_report.json'):
    """å¯¼å‡ºé‡å¤æ™¯ç‚¹æŠ¥å‘Šä¸ºJSON"""
    data_dir = Path('data')
    output_path = Path(output_file)
    output_path.parent.mkdir(exist_ok=True)
    
    report = {}
    
    for json_file in sorted(data_dir.glob('spots_*.json')):
        city = json_file.stem.replace('spots_', '')
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                spots = json.load(f)
        except:
            continue
        
        duplicates = check_duplicate_spots(spots, city)
        
        if duplicates:
            report[city] = {
                'total_spots': len(spots),
                'duplicate_groups': len(duplicates),
                'duplicates': [{
                    'spot1': dup['spot1'],
                    'spot2': dup['spot2'],
                    'index1': dup['index1'],
                    'index2': dup['index2'],
                    'similarity': round(dup['similarity'], 3),
                    'distance_meters': round(dup['distance'], 1) if dup['distance'] else None,
                    'reason': dup['reason']
                } for dup in duplicates]
            }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… é‡å¤æ™¯ç‚¹æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
    return report

def interactive_cleanup_duplicates(city_name):
    """
    äº¤äº’å¼æ¸…ç†é‡å¤æ™¯ç‚¹ï¼ˆä¸ä¼šå½±å“ä»»ä½•å·²æœ‰åŠŸèƒ½ï¼‰
    """
    data_dir = Path('data')
    json_file = data_dir / f'spots_{city_name}.json'

    if not json_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°åŸå¸‚æ•°æ®æ–‡ä»¶: {json_file}")
        return

    with open(json_file, 'r', encoding='utf-8') as f:
        spots = json.load(f)

    duplicates = check_duplicate_spots(spots, city_name)

    if not duplicates:
        print("âœ… æœªå‘ç°é‡å¤æ™¯ç‚¹ï¼Œæ— éœ€æ¸…ç†")
        return

    print("=" * 80)
    print(f"ğŸ§¹ äº¤äº’å¼é‡å¤æ™¯ç‚¹æ¸…ç†ï¼š{city_name}")
    print("=" * 80)

    to_delete = set()

    for idx, dup in enumerate(duplicates, 1):
        i, j = dup['index1'], dup['index2']

        # å¦‚æœå·²è¢«åˆ é™¤ï¼Œè·³è¿‡
        if i in to_delete or j in to_delete:
            continue

        spot_a = spots[i]
        spot_b = spots[j]

        print(f"\n[{idx}] ç–‘ä¼¼é‡å¤æ™¯ç‚¹")
        print("-" * 80)
        print(f"A [{i}] {spot_a.get('name')}")
        print(f"   rating={spot_a.get('rating')}  desc_len={len(spot_a.get('description',''))}")
        print(f"B [{j}] {spot_b.get('name')}")
        print(f"   rating={spot_b.get('rating')}  desc_len={len(spot_b.get('description',''))}")
        print(f"åˆ¤æ–­ä¾æ®: {dup['reason']}")

        choice = input("æ“ä½œ ([1]åˆ A / [2]åˆ B / [s]è·³è¿‡ / [q]é€€å‡º): ").strip().lower()

        if choice == '1':
            to_delete.add(i)
            print("ğŸ—‘ï¸ å·²æ ‡è®°åˆ é™¤ A")
        elif choice == '2':
            to_delete.add(j)
            print("ğŸ—‘ï¸ å·²æ ‡è®°åˆ é™¤ B")
        elif choice == 'q':
            print("â›” å·²é€€å‡ºæ¸…ç†æµç¨‹")
            break
        else:
            print("â­ï¸ è·³è¿‡è¯¥ç»„")

    if not to_delete:
        print("âš ï¸ æœªé€‰æ‹©åˆ é™¤ä»»ä½•æ™¯ç‚¹")
        return

    # å¤‡ä»½
    backup = json_file.with_suffix('.json.bak')
    json_file.replace(backup)
    print(f"\nğŸ“¦ åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º: {backup.name}")

    # åå‘åˆ é™¤ï¼Œä¿è¯ç´¢å¼•å®‰å…¨
    for index in sorted(to_delete, reverse=True):
        del spots[index]

    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(spots, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {len(to_delete)} ä¸ªæ™¯ç‚¹")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == 'analyze':
            # å…¨é¢åˆ†ææ‰€æœ‰åŸå¸‚
            analyze_data_quality()
        
        elif command == 'duplicates':
            # åªæ˜¾ç¤ºé‡å¤æ™¯ç‚¹æŠ¥å‘Š
            city = sys.argv[2] if len(sys.argv) > 2 else None
            generate_duplicate_report(city)
        
        elif command == 'cleanup':
            # ä¸ºç‰¹å®šåŸå¸‚ç”Ÿæˆæ¸…ç†å»ºè®®
            if len(sys.argv) < 3:
                print("è¯·æŒ‡å®šåŸå¸‚åç§°ï¼Œä¾‹å¦‚: python check_data_quality.py cleanup beijing")
            else:
                city = sys.argv[2]
                generate_cleanup_suggestions(city)
        
        elif command == 'export':
            # å¯¼å‡ºæŠ¥å‘Šä¸ºJSON
            export_duplicates_json()
        
        elif command == 'interactive-clean':
            if len(sys.argv) < 3:
                print("è¯·æŒ‡å®šåŸå¸‚åç§°ï¼Œä¾‹å¦‚: python check_data_quality.py interactive-clean beijing")
            else:
                city = sys.argv[2]
                interactive_cleanup_duplicates(city)

        
        else:
            print("æœªçŸ¥å‘½ä»¤ã€‚å¯ç”¨å‘½ä»¤:")
            print("  analyze    - å…¨é¢åˆ†ææ‰€æœ‰åŸå¸‚æ•°æ®è´¨é‡")
            print("  duplicates [city] - æ˜¾ç¤ºé‡å¤æ™¯ç‚¹æŠ¥å‘Š")
            print("  cleanup <city> - ä¸ºç‰¹å®šåŸå¸‚ç”Ÿæˆæ¸…ç†å»ºè®®")
            print("  export     - å¯¼å‡ºé‡å¤æ™¯ç‚¹æŠ¥å‘Šä¸ºJSON")
    else:
        # é»˜è®¤æ‰§è¡Œå…¨é¢åˆ†æ
        analyze_data_quality()
